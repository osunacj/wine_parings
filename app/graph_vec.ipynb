{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "from llama_index.core import Response\n",
    "from llama_index.core.evaluation import (\n",
    "    FaithfulnessEvaluator,\n",
    "    DatasetGenerator,\n",
    "    RelevancyEvaluator,\n",
    "    EvaluationResult,\n",
    "    CorrectnessEvaluator,\n",
    "    AnswerRelevancyEvaluator,\n",
    "    ContextRelevancyEvaluator,\n",
    "    BatchEvalRunner,\n",
    "    RetrieverEvaluator,\n",
    "    SemanticSimilarityEvaluator,\n",
    "    QueryResponseDataset,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from notebooks.helpers.bot.promtps import  (ANSWER_REL_EVAL_TEMPLATE,\n",
    "    question_gen_query,\n",
    "    EVALUATION_CORRECTNESS_SYSTEM_TEMPLATE,\n",
    "    FAITH_EVAL_TEMPLATE,\n",
    "    CONTEXT_REL_PROMPT, EVALUATION_CORRECTNESS_SYSTEM_TEMPLATE)\n",
    "from evaluate_pairings import prepare_evalution_qa, parse_evalutions, get_qr_pairs, evaluate_correctness, evaluate_faithfulness, default_parser, evaluate_ans_relevancy, evaluate_context_relevancy, evaluate_relevancy\n",
    "from app.notebooks.helpers.bot.kg_generation import create_kg_triplets\n",
    "from notebooks.helpers.bot.bot import (\n",
    "    get_chat_engine,\n",
    "    get_query_engine,\n",
    "    load_llm,\n",
    "    load_embedding_model,\n",
    "    setup_index_and_storage,\n",
    "    generate_pairings_documents,\n",
    "    service,\n",
    ")\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_eval_df(query: str, response: Response, eval_result: Response) -> None:\n",
    "    eval_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Query\": query,\n",
    "            \"Response\": str(response),\n",
    "            \"Source\": (' '.join(eval_result.contexts)[:1000] + \"...\"),\n",
    "            \"Evaluation Result\": eval_result.passing,\n",
    "        },\n",
    "        index = [0]\n",
    "    )\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    display(eval_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_triplets(text):\n",
    "    triplets = []\n",
    "    rows = text.split('\\n')\n",
    "    for row in rows[:-1]:\n",
    "        triplet = row.split('**')\n",
    "        triplets.append((triplet[0], triplet[1], triplet[2]))\n",
    "    return triplets\n",
    "\n",
    "kg_triplets = []\n",
    "\n",
    "KG = create_kg_triplets(sample_size=10, for_model=False)\n",
    "G = nx.DiGraph()\n",
    "for _, row in KG.iterrows():\n",
    "    triplets = parse_triplets(row['triplets'])\n",
    "    for triplet in triplets:\n",
    "        G.add_edge(triplet[0], triplet[2], label=triplet[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KG = create_kg_triplets()\n",
    "# kg_pairings = KG.apply(generate_pairings_documents, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(kg_pairings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(kg_pairings.iloc[0].get_content(metadata_mode='llm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = load_llm('openai3.5')\n",
    "embed_model = load_embedding_model(\"openai3\")\n",
    "service_context = service(llm=llm, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-1242' coro=<tqdm_asyncio.gather.<locals>.wrap_awaitable() done, defined at /home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/tqdm/asyncio.py:75> exception=RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-rSW8VTGvcLlOXkwlKwR4NiVf on tokens per min (TPM): Limit 60000, Used 59640, Requested 3577. Please try again in 3.217s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/asyncio/tasks.py\", line 280, in __step\n",
      "    result = coro.send(None)\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/tqdm/asyncio.py\", line 76, in wrap_awaitable\n",
      "    return i, await f\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/llama_index/core/evaluation/batch_runner.py\", line 43, in eval_worker\n",
      "    await evaluator.aevaluate(\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/llama_index/core/evaluation/answer_relevancy.py\", line 123, in aevaluate\n",
      "    eval_response = await self._llm.apredict(\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 114, in async_wrapper\n",
      "    self.span_drop(*args, id=id, err=e, **kwargs)\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 77, in span_drop\n",
      "    h.span_drop(*args, id=id, err=err, **kwargs)\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/llama_index/core/instrumentation/span_handlers/base.py\", line 47, in span_drop\n",
      "    self.prepare_to_drop_span(*args, id=id, err=err, **kwargs)\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/llama_index/core/instrumentation/span_handlers/null.py\", line 35, in prepare_to_drop_span\n",
      "    raise err\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 112, in async_wrapper\n",
      "    result = await func(*args, **kwargs)\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/llama_index/core/llms/llm.py\", line 494, in apredict\n",
      "    chat_response = await self.achat(messages)\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/llama_index/core/llms/callbacks.py\", line 68, in wrapped_async_llm_chat\n",
      "    f_return_val = await f(_self, messages, **kwargs)\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/llama_index/llms/openai/base.py\", line 536, in achat\n",
      "    return await achat_fn(messages, **kwargs)\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/tenacity/_asyncio.py\", line 88, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/tenacity/_asyncio.py\", line 47, in __call__\n",
      "    do = self.iter(retry_state=retry_state)\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/tenacity/__init__.py\", line 325, in iter\n",
      "    raise retry_exc.reraise()\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/tenacity/__init__.py\", line 158, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/_base.py\", line 437, in result\n",
      "    return self.__get_result()\n",
      "  File \"/usr/lib/python3.8/concurrent/futures/_base.py\", line 389, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/tenacity/_asyncio.py\", line 50, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/llama_index/llms/openai/base.py\", line 581, in _achat\n",
      "    response = await aclient.chat.completions.create(\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/openai/resources/chat/completions.py\", line 1334, in create\n",
      "    return await self._post(\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/openai/_base_client.py\", line 1738, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/openai/_base_client.py\", line 1441, in request\n",
      "    return await self._request(\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/openai/_base_client.py\", line 1517, in _request\n",
      "    return await self._retry_request(\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
      "    return await self._request(\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/openai/_base_client.py\", line 1517, in _request\n",
      "    return await self._retry_request(\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
      "    return await self._request(\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/openai/_base_client.py\", line 1517, in _request\n",
      "    return await self._retry_request(\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/openai/_base_client.py\", line 1563, in _retry_request\n",
      "    return await self._request(\n",
      "  File \"/home/carlosjosuna/.local/share/virtualenvs/wine_parings-Y4sTaGQc/lib/python3.8/site-packages/openai/_base_client.py\", line 1532, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-rSW8VTGvcLlOXkwlKwR4NiVf on tokens per min (TPM): Limit 60000, Used 59640, Requested 3577. Please try again in 3.217s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
     ]
    }
   ],
   "source": [
    "storage_context, kg_index = setup_index_and_storage(\n",
    "    service=service_context,\n",
    "    kg_pairings=None,\n",
    "    show_progress=True,\n",
    "    force=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_eval = load_llm(\"openai3.5\")\n",
    "embed_model_eval = load_embedding_model(\"openai3\")\n",
    "service_context_eval = service(llm=llm_eval, embed_model=embed_model_eval)\n",
    "relevancy_eval = RelevancyEvaluator(service_context=service_context_eval)\n",
    "faithfulness_eval = FaithfulnessEvaluator(\n",
    "    service_context=service_context, eval_template=FAITH_EVAL_TEMPLATE\n",
    ")\n",
    "semantic_eval = SemanticSimilarityEvaluator(service_context=service_context_eval)\n",
    "answer_eval = AnswerRelevancyEvaluator(\n",
    "    service_context=service_context_eval,\n",
    "    eval_template=ANSWER_REL_EVAL_TEMPLATE,\n",
    "    score_threshold=3.0,\n",
    ")\n",
    "context_eval = ContextRelevancyEvaluator(\n",
    "    service_context=service_context_eval, eval_template = CONTEXT_REL_PROMPT\n",
    ")\n",
    "correctness_eval = CorrectnessEvaluator(\n",
    "    service_context=service_context_eval,\n",
    "    parser_function=default_parser,\n",
    "    eval_template=EVALUATION_CORRECTNESS_SYSTEM_TEMPLATE,\n",
    ")\n",
    "\n",
    "\n",
    "CHAT_MODE = \"context\"\n",
    "RETRIEVER_MODE = \"hybrid\"\n",
    "RESPONSE_MODE = \"compact\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHAT_MODE != 'simple':\n",
    "    runner = BatchEvalRunner(\n",
    "        {\n",
    "            \"semantic\": semantic_eval,\n",
    "            \"answer_relevancy\": answer_eval,\n",
    "            \"context_relevancy\": context_eval,\n",
    "            \"relevancy\": relevancy_eval,\n",
    "            # \"correctness\": correctness_eval,\n",
    "\n",
    "        },\n",
    "        workers=5,\n",
    "        show_progress=True,\n",
    "    )\n",
    "else:\n",
    "    runner = BatchEvalRunner(\n",
    "    {\n",
    "        \"semantic\": semantic_eval,\n",
    "        \"answer_relevancy\": answer_eval,\n",
    "        \"relevancy\": relevancy_eval,\n",
    "        # \"correctness\": correctness_eval,\n",
    "    },\n",
    "    workers=6,\n",
    "    show_progress=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = get_query_engine(\n",
    "    kg_index,\n",
    "    chat_mode=CHAT_MODE,\n",
    "    retriver_mode=RETRIEVER_MODE,\n",
    "    response_mode=RESPONSE_MODE,\n",
    "    use_global_node_triplets=False,\n",
    "    max_keywords_per_query=10,\n",
    "    num_chunks_per_query=10,\n",
    "    similarity_top_k=4,\n",
    "    graph_store_query_depth=2,\n",
    "    include_text=False,  # Do not include text of the node into the model\n",
    "    verbose = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying Knowledge Graph: 100%|██████████| 162/162 [10:56<00:00,  4.05s/it]\n"
     ]
    }
   ],
   "source": [
    "queries, references = get_qr_pairs()\n",
    "\n",
    "if CHAT_MODE == 'simple':\n",
    "    responses_strs = [\n",
    "        llm.complete(query).text\n",
    "        for query in tqdm(queries, total=len(queries), desc=\"Responses from Model\")\n",
    "    ]\n",
    "    contexts = [['']] * len(responses_strs)\n",
    "else:\n",
    "    responses = [\n",
    "        query_engine.query(query)\n",
    "        for query in tqdm(queries, total=len(queries), desc=\"Querying Knowledge Graph\")\n",
    "    ]\n",
    "    responses_strs = [response_ty.response for response_ty in responses]\n",
    "    contexts = [[node.get_content()] for response in responses for node in response.source_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 350/648 [01:49<01:52,  2.65it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.6063655929045642 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-rSW8VTGvcLlOXkwlKwR4NiVf on tokens per min (TPM): Limit 60000, Used 57091, Requested 3496. Please try again in 587ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      " 62%|██████▏   | 402/648 [02:11<01:54,  2.14it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.0576832651751904 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-rSW8VTGvcLlOXkwlKwR4NiVf on tokens per min (TPM): Limit 60000, Used 56881, Requested 3585. Please try again in 466ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      " 62%|██████▏   | 403/648 [02:11<01:56,  2.10it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.8075021148288487 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-rSW8VTGvcLlOXkwlKwR4NiVf on tokens per min (TPM): Limit 60000, Used 56843, Requested 3604. Please try again in 447ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      " 69%|██████▉   | 446/648 [02:30<01:44,  1.92it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.676456267685029 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-rSW8VTGvcLlOXkwlKwR4NiVf on tokens per min (TPM): Limit 60000, Used 59583, Requested 3527. Please try again in 3.11s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      " 75%|███████▍  | 484/648 [02:45<01:09,  2.37it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.1508789222866196 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-rSW8VTGvcLlOXkwlKwR4NiVf on tokens per min (TPM): Limit 60000, Used 57250, Requested 3493. Please try again in 743ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      " 78%|███████▊  | 503/648 [02:53<01:06,  2.17it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.49778001040432096 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-rSW8VTGvcLlOXkwlKwR4NiVf on tokens per min (TPM): Limit 60000, Used 56783, Requested 3575. Please try again in 358ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      " 78%|███████▊  | 505/648 [02:53<01:06,  2.15it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.346621574276446 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-rSW8VTGvcLlOXkwlKwR4NiVf on tokens per min (TPM): Limit 60000, Used 56919, Requested 3523. Please try again in 442ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      " 80%|███████▉  | 516/648 [02:57<00:41,  3.19it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.14625441027063946 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-rSW8VTGvcLlOXkwlKwR4NiVf on tokens per min (TPM): Limit 60000, Used 57124, Requested 3339. Please try again in 463ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      " 81%|████████  | 525/648 [03:01<00:53,  2.31it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.6471453806914568 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-rSW8VTGvcLlOXkwlKwR4NiVf on tokens per min (TPM): Limit 60000, Used 57106, Requested 3799. Please try again in 905ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      " 84%|████████▍ | 547/648 [03:10<01:07,  1.49it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5470055927144274 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-rSW8VTGvcLlOXkwlKwR4NiVf on tokens per min (TPM): Limit 60000, Used 57295, Requested 3633. Please try again in 928ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      " 87%|████████▋ | 566/648 [03:19<00:38,  2.15it/s]Retrying llama_index.llms.openai.base.OpenAI._achat in 0.5157572156188825 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-rSW8VTGvcLlOXkwlKwR4NiVf on tokens per min (TPM): Limit 60000, Used 58843, Requested 3569. Please try again in 2.412s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}.\n",
      "100%|██████████| 648/648 [03:57<00:00,  2.73it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_results = runner.evaluate_response_strs(\n",
    "    queries=queries,\n",
    "    response_strs = responses_strs,\n",
    "    contexts_list = contexts,\n",
    "    reference=references,  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Faithfulness: 100%|██████████| 162/162 [01:27<00:00,  1.84it/s]\n",
      "Calculating Correctness: 100%|██████████| 162/162 [04:08<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_faithfulness(faithfulness_eval, queries, contexts, responses):\n",
    "    results = []\n",
    "    for query, context, response in tqdm(\n",
    "        zip(queries, contexts, responses),\n",
    "        total=len(responses),\n",
    "        desc=\"Calculating Faithfulness\",\n",
    "    ):\n",
    "        try:\n",
    "            evaluation = faithfulness_eval.evaluate(\n",
    "                query=query, response=response, contexts=context\n",
    "            )\n",
    "            results.append(evaluation)\n",
    "        except:\n",
    "            results.append(None)\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def evaluate_correctness(correctness_eval, queries, references, responses):\n",
    "    results = []\n",
    "    for query, reference, response in tqdm(\n",
    "        zip(queries, references, responses),\n",
    "        total=len(responses),\n",
    "        desc=\"Calculating Correctness\",\n",
    "    ):\n",
    "        try:\n",
    "            evaluation = correctness_eval.evaluate(\n",
    "                query=query, response=response, referece=reference\n",
    "            )\n",
    "            results.append(evaluation)\n",
    "        except:\n",
    "            results.append(None)\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "eval_results[\"faithfulness\"] = evaluate_faithfulness(\n",
    "    faithfulness_eval=faithfulness_eval,\n",
    "    queries=queries,\n",
    "    contexts = contexts,\n",
    "    responses=responses_strs,\n",
    ")\n",
    "eval_results[\"correctness\"] = evaluate_correctness(\n",
    "    correctness_eval,\n",
    "    queries,\n",
    "    references,\n",
    "    responses=responses_strs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic Score: 0.8921377954665135\n",
      "answer_relevancy Score: 0.9938271604938271\n",
      "context_relevancy Score: 0.5\n",
      "relevancy Score: 0.9567901234567902\n",
      "faithfulness Score: 0.9876543209876543\n",
      "correctness Score: 0.6296296296296297\n"
     ]
    }
   ],
   "source": [
    "parse_evalutions(\n",
    "    eval_results=eval_results,\n",
    "    model=\"gpt-3.5\",\n",
    "    embedding_model=\"gpt-3.5\",\n",
    "    chat_mode=CHAT_MODE,\n",
    "    retriever_mode=RETRIEVER_MODE,\n",
    "    response_mode=RESPONSE_MODE,\n",
    "    queries=queries,\n",
    "    responses=references,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_engine = get_chat_engine(\n",
    "    kg_index,\n",
    "    response_mode='compact',\n",
    "    retriver_mode='hybrid',\n",
    "    chat_mode='context',\n",
    "    use_global_node_triplets=True,\n",
    "    max_keywords_per_query=10,\n",
    "    num_chunks_per_query=10,\n",
    "    similarity_top_k=4,\n",
    "    graph_store_query_depth=2,\n",
    "    include_text=False,\n",
    ")\n",
    "\n",
    "chat_engine.reset()\n",
    "\n",
    "def chat_with_query(query):\n",
    "    response = chat_engine.chat(query)\n",
    "    eval_response = evaluator.evaluate_response(query, response)\n",
    "    return eval_response, query\n",
    "\n",
    "def print_chat_interface(response, query):\n",
    "    interface = f\"\"\"\n",
    "    QUERY: {query}\\n\n",
    "    --------------------------\\n\n",
    "    Context:\\n {\" \".join(response.contexts)}\\n\n",
    "    ---------------------------\\n\n",
    "    Response: {response.response.replace('**', '')}\n",
    "    \"\"\"\n",
    "    print(interface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "queries, responses = get_qr_pairs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much does the Casaloste 2011 Chianti Classico wine cost?\n"
     ]
    }
   ],
   "source": [
    "indx = random.randint(a=0, b=160)\n",
    "query = queries[indx]\n",
    "reference = responses[indx]\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model.get_query_embedding(\"What three wines best pair with a ribeye steak and as a side dish baked potatoes with butter and mushrooms?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;32mExtracted keywords: ['wines', 'potatoes', 'baked', 'ribeye', 'ribeye steak', 'baked potatoes', 'side', 'mushrooms', 'pair', 'steak', 'three wines', 'side dish', 'dish', 'butter', 'three']\n",
      "\u001b[0m\u001b[1;3;34mKG context:\n",
      "The following are knowledge sequence in max depth 2 in the form of directed graph like:\n",
      "`subject -[predicate]->, object, <-[predicate_next_hop]-, object_next_hop ...`\n",
      "('steak roquefort', 'pairs_with', 'Pinot Noir San Francisco Bay, Central Coast, California, USA')\n",
      "('steak and drunken mushroom', 'pairs_with', 'White Blend Sierra Foothills, California, USA')\n",
      "('steak and drunken mushroom', 'pairs_with', 'White Blend Columbia Valley, Washington, USA')\n",
      "('steak roquefort', 'pairs_with', 'White Blend Southwest France, France')\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "eval_response, query = chat_with_query(\"What three wines best pair with a ribeye steak and as a side dish baked potatoes with butter and mushrooms?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    QUERY: What three wines best pair with a ribeye steak and as a side dish baked potatoes with butter and mushrooms?\n",
      "\n",
      "    --------------------------\n",
      "\n",
      "    Context:\n",
      " The following are knowledge sequence in max depth 2 in the form of directed graph like:\n",
      "`subject -[predicate]->, object, <-[predicate_next_hop]-, object_next_hop ...`\n",
      "('steak roquefort', 'pairs_with', 'Pinot Noir San Francisco Bay, Central Coast, California, USA')\n",
      "('steak and drunken mushroom', 'pairs_with', 'White Blend Sierra Foothills, California, USA')\n",
      "('steak and drunken mushroom', 'pairs_with', 'White Blend Columbia Valley, Washington, USA')\n",
      "('steak roquefort', 'pairs_with', 'White Blend Southwest France, France')\n",
      "\n",
      "    ---------------------------\n",
      "\n",
      "    Response: When pairing wines with a ribeye steak and baked potatoes with butter and mushrooms, you want to consider the rich and savory flavors of the dish. Here are three wine options that would complement this meal:\n",
      "\n",
      "1. Cabernet Sauvignon: \n",
      "   - Why: Cabernet Sauvignon is a classic pairing with steak due to its bold tannins and flavors of dark fruits, which can stand up to the richness of the ribeye. It also complements the earthy flavors of the mushrooms.\n",
      "   - Suggested Region: Napa Valley, California, USA\n",
      "\n",
      "2. Syrah/Shiraz:\n",
      "   - Why: Syrah/Shiraz offers a mix of dark fruit flavors, spice, and a meaty character that can enhance the flavors of the steak and mushrooms.\n",
      "   - Suggested Region: Barossa Valley, Australia\n",
      "\n",
      "3. Chardonnay:\n",
      "   - Why: A full-bodied Chardonnay with oak aging can provide a creamy texture and flavors of butter and vanilla that complement the baked potatoes with butter. It can also balance the richness of the steak.\n",
      "   - Suggested Region: Sonoma Coast, California, USA\n",
      "\n",
      "These wine options should enhance the dining experience and bring out the best in both the steak and the side dish. Enjoy your meal!\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print_chat_interface(eval_response, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.96it/s]\n",
      "Calculating Faithfulness: 100%|██████████| 1/1 [00:00<00:00, 732.25it/s]\n",
      "Calculating Correctness: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "queries = [query]\n",
    "response_strs = [eval_response.response]\n",
    "contexts_list = [eval_response.contexts]\n",
    "references = [\"For a duck dish cooked in a traditional Japanese 'donabe' hot pot with a collection of vegetables, you'll want a wine that can complement the rich and savory flavors of the duck while also harmonizing with the umami notes from the vegetables and broth. One excellent wine pairing for this dish would be a Pinot Noir from Burgundy, France. Pinot Noir from Burgundy is known for its elegant and delicate flavors, with notes of red fruit, earthiness, and a silky texture. The wine's medium body and balanced acidity can complement the richness of the duck while also enhancing the flavors of the vegetables in the hot pot. Another great option would be a Riesling from the Mosel region, Germany. A crisp and slightly off-dry Riesling can provide a refreshing contrast to the savory duck and umami-rich vegetables. The wine's acidity and hint of sweetness can balance the flavors of the dish and cleanse the palate between bites. These wine choices can elevate the dining experience by creating a harmonious balance of flavors between the duck hot pot and the wines, enhancing the overall enjoyment of the meal with a touch of elegance and sophistication.\"\n",
    "]\n",
    "\n",
    "eval_results = runner.evaluate_response_strs(\n",
    "    queries= queries,\n",
    "    response_strs = response_strs,\n",
    "    contexts_list = contexts_list,\n",
    "    reference=references,  # type: ignore\n",
    ")\n",
    "\n",
    "\n",
    "eval_results[\"faithfulness\"] = evaluate_faithfulness(\n",
    "    faithfulness_eval=faithfulness_eval,\n",
    "    queries=queries,\n",
    "    contexts = contexts_list,\n",
    "    responses=response_strs,\n",
    ")\n",
    "eval_results[\"correctness\"] = evaluate_correctness(\n",
    "    correctness_eval,\n",
    "    queries,\n",
    "    references,\n",
    "    responses=response_strs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic 0.9202498699259001 True\n",
      "answer_relevancy 1.0 None\n",
      "context_relevancy None None\n",
      "relevancy 0.0 False\n",
      "faithfulness 0.0 False\n",
      "correctness 5.0 True\n"
     ]
    }
   ],
   "source": [
    "for key, val in eval_results.items():\n",
    "    resp =  dict(val[0])\n",
    "    print(key, resp['score'], resp['passing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_response, query = chat_with_query(\"From these wines which one is the most expensive?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_chat_interface(eval_response, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_response, query = chat_with_query(\"From the Bordeaux wine provided which one is considered a budget wine?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_chat_interface(eval_response, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = kg_index.get_networkx_graph(limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(notebook = True, cdn_resources = \"remote\",\n",
    "                # bgcolor = \"#222222\",\n",
    "                font_color = \"black\",\n",
    "                height = \"750px\",\n",
    "                width = \"100%\",\n",
    "                select_menu = True,\n",
    "                filter_menu = True,\n",
    ")\n",
    "net.show_buttons(filter_=\"physics\")\n",
    "net.from_nx(G)\n",
    "net.show(\"nx.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(G, seed=42, k=1.5)\n",
    "labels = nx.get_edge_attributes(G, 'label')\n",
    "plt.figure(figsize=(20, 20))\n",
    "nx.draw(G, pos, font_size=8, node_size=200, node_color='lightblue', edge_color='gray', alpha=0.6)\n",
    "# nx.draw_networkx_edge_labels(G, pos, font_size=3, label_pos=0.3, verticalalignment='baseline')\n",
    "plt.title('Knowledge Graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in G.edges(data=True):\n",
    "    print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_dataset_path = Path(\"./app/data/evaluation/evaluation_evolved.json\")\n",
    "\n",
    "data_generator = DatasetGenerator.from_documents(\n",
    "    kg_pairings.sample(n=3000),\n",
    "    service_context=service_context,\n",
    "    question_gen_query=question_gen_query,\n",
    "    num_questions_per_chunk=2,\n",
    "    show_progress = True\n",
    ")\n",
    "eval_dataset = data_generator.generate_dataset_from_nodes(130)\n",
    "eval_dataset.save_json(eval_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wine_parings-Y4sTaGQc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
