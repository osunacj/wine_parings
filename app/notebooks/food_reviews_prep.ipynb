{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from helpers.prep.foodbert_norm import RecipeNormalizer\n",
    "\n",
    "# from operator import itemgetter\n",
    "# from collections import Counter, OrderedDict\n",
    "\n",
    "# from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# from nltk.stem import SnowballStemmer\n",
    "# from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "BASE_PATH = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the wine dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the food dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(231637, 12)\n"
     ]
    }
   ],
   "source": [
    "food_review_dataset = pd.read_csv(BASE_PATH + '/food_reviews/RAW_recipes.csv')\n",
    "print(food_review_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(df):\n",
    "    df[['calories','total fat (PDV)','sugar (PDV)','sodium (PDV)','protein (PDV)','saturated fat (PDV)','carbohydrates (PDV)']] = df.nutrition.str.split(\",\",expand=True) \n",
    "    df['calories'] =  df['calories'].apply(lambda x: x.replace('[','')) \n",
    "    df['carbohydrates (PDV)'] =  df['carbohydrates (PDV)'].apply(lambda x: x.replace(']','')) \n",
    "    df[['calories','total fat (PDV)','sugar (PDV)','sodium (PDV)','protein (PDV)','saturated fat (PDV)','carbohydrates (PDV)']] = df[['calories','total fat (PDV)','sugar (PDV)','sodium (PDV)','protein (PDV)','saturated fat (PDV)','carbohydrates (PDV)']].astype('float')\n",
    "    df.drop(['minutes', 'contributor_id', 'submitted', 'tags', 'nutrition', 'n_steps', 'n_ingredients'], inplace=True, axis=1)\n",
    "    return df\n",
    "\n",
    "food_review_dataset = remove_columns(food_review_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['make a choice and proceed with recipe', 'depending on size of squash , cut into half or fourths', 'remove seeds', 'for spicy squash , drizzle olive oil or melted butter over each cut squash piece', 'season with mexican seasoning mix ii', 'for sweet squash , drizzle melted honey , butter , grated piloncillo over each cut squash piece', 'season with sweet mexican spice mix', 'bake at 350 degrees , again depending on size , for 40 minutes up to an hour , until a fork can easily pierce the skin', 'be careful not to burn the squash especially if you opt to use sugar or butter', 'if you feel more comfortable , cover the squash with aluminum foil the first half hour , give or take , of baking', 'if desired , season with salt']\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_review_dataset.steps.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231637/231637 [00:03<00:00, 65863.09it/s]\n",
      "100%|██████████| 14899/14899 [00:13<00:00, 1137.79it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_ingredients(all_raw_ingredients):\n",
    "    list_ingredients = []\n",
    "    for ingredients in tqdm(all_raw_ingredients, total=len(all_raw_ingredients)):\n",
    "        for ingredient in eval(ingredients):\n",
    "            if \" and \" in ingredient or \" or \" in ingredient:\n",
    "                ingredient = ingredient.replace(\" and \", \" \").split(\" \")\n",
    "                for ingre in ingredient:\n",
    "                    list_ingredients.append(ingre)\n",
    "            else:\n",
    "                list_ingredients.append(ingredient)\n",
    "\n",
    "    list_ingredients = list(dict.fromkeys(list_ingredients))\n",
    "    ingredient_normalizer = RecipeNormalizer(lemmatization_types=[\"NOUN\"])\n",
    "\n",
    "    cleaned_ingredients = ingredient_normalizer.normalize_ingredients(list_ingredients)\n",
    "    cleaned_ingredients = ingredient_normalizer.read_and_write_ingredients(\n",
    "        cleaned_ingredients\n",
    "    )\n",
    "\n",
    "    return cleaned_ingredients\n",
    "\n",
    "\n",
    "cleaned_ingredients = extract_ingredients(food_review_dataset.ingredients.to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:15<00:00,  6.60it/s]\n"
     ]
    }
   ],
   "source": [
    "def normalize_instructions(instructions_list):\n",
    "    normalized_instructions = []\n",
    "    instruction_normalizer = RecipeNormalizer()\n",
    "    for instructions in tqdm(instructions_list, total=len(instructions_list)):\n",
    "        if instructions is np.nan:\n",
    "            normalized_instructions.append(None)\n",
    "            continue\n",
    "\n",
    "        if type(eval(instructions)) == str:\n",
    "            instruction_text = [instructions]\n",
    "        else:\n",
    "            instruction_text = [step.strip() for step in eval(instructions)]\n",
    "\n",
    "        normalized_instructions.append(\n",
    "            instruction_normalizer.normalize_instruction(instruction_text)\n",
    "        )\n",
    "    return normalized_instructions\n",
    "\n",
    "\n",
    "normalized_instructions_token = normalize_instructions(food_review_dataset['steps'].to_numpy()[:500])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:15<00:00,  6.60it/s]\n",
      "100%|██████████| 500/500 [01:21<00:00,  6.10it/s]\n"
     ]
    }
   ],
   "source": [
    "normalized_name_token = normalize_instructions(food_review_dataset['name'].to_numpy())\n",
    "normalized_description_token = normalize_instructions(food_review_dataset['description'].to_numpy()[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_review_dataset.drop(['name', 'steps', 'description'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27317/2779381408.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  food_review_dataset['clean_instructions'][:500] = normalized_instructions_token\n",
      "/tmp/ipykernel_27317/2779381408.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  food_review_dataset['clean_description'][:500] = normalized_description_token\n",
      "/tmp/ipykernel_27317/2779381408.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  food_review_dataset['clean_name'][:500] = normalized_name_token\n"
     ]
    }
   ],
   "source": [
    "food_review_dataset[['clean_instructions', 'clean_description', 'clean_name']] = None\n",
    "food_review_dataset['clean_instructions'][:500] = normalized_instructions_token\n",
    "food_review_dataset['clean_description'][:500] = normalized_description_token\n",
    "food_review_dataset['clean_name'][:500] = normalized_name_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>calories</th>\n",
       "      <th>total fat (PDV)</th>\n",
       "      <th>sugar (PDV)</th>\n",
       "      <th>sodium (PDV)</th>\n",
       "      <th>protein (PDV)</th>\n",
       "      <th>saturated fat (PDV)</th>\n",
       "      <th>carbohydrates (PDV)</th>\n",
       "      <th>clean_instructions</th>\n",
       "      <th>clean_description</th>\n",
       "      <th>clean_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137739</td>\n",
       "      <td>['winter squash', 'mexican seasoning', 'mixed ...</td>\n",
       "      <td>51.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>' make a choice and proceed with recipe',' dep...</td>\n",
       "      <td>autumn is my favorite time of year to cook! th...</td>\n",
       "      <td>arriba baked winter_squash mexican style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31490</td>\n",
       "      <td>['prepared pizza crust', 'sausage patty', 'egg...</td>\n",
       "      <td>173.4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>' preheat oven to 425 degree f',' press dough ...</td>\n",
       "      <td>this recipe calls for the crust to be prebaked...</td>\n",
       "      <td>a bit different breakfast pizza</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                        ingredients  calories  \\\n",
       "0  137739  ['winter squash', 'mexican seasoning', 'mixed ...      51.5   \n",
       "1   31490  ['prepared pizza crust', 'sausage patty', 'egg...     173.4   \n",
       "\n",
       "   total fat (PDV)  sugar (PDV)  sodium (PDV)  protein (PDV)  \\\n",
       "0              0.0         13.0           0.0            2.0   \n",
       "1             18.0          0.0          17.0           22.0   \n",
       "\n",
       "   saturated fat (PDV)  carbohydrates (PDV)  \\\n",
       "0                  0.0                  4.0   \n",
       "1                 35.0                  1.0   \n",
       "\n",
       "                                  clean_instructions  \\\n",
       "0  ' make a choice and proceed with recipe',' dep...   \n",
       "1  ' preheat oven to 425 degree f',' press dough ...   \n",
       "\n",
       "                                   clean_description  \\\n",
       "0  autumn is my favorite time of year to cook! th...   \n",
       "1  this recipe calls for the crust to be prebaked...   \n",
       "\n",
       "                                 clean_name  \n",
       "0  arriba baked winter_squash mexican style  \n",
       "1           a bit different breakfast pizza  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_review_dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"' preheat oven to 425 degree f',' press dough into the bottom and side of a 12 inch pizza pan',' bake for 5 minute until set but not browned',' cut sausage into small piece',' whisk egg and milk in a bowl until frothy',' spoon sausage over baked crust and sprinkle with cheese',' pour egg mixture slowly over sausage and cheese', 's p to taste',' bake 15- 20 minute or until egg are set and crust is brown'\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_review_dataset.clean_instructions.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_review_dataset.to_csv(BASE_PATH + '/produce/food_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.prep.frequencies import FrquencyExtractor\n",
    "\n",
    "f_extractor = FrquencyExtractor(clean_ingredients=cleaned_ingredients, clean_sentences = normalized_instructions_token)\n",
    "\n",
    "ingredient_count = f_extractor.count_all_ingredients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training our Word Embeddings\n",
    "\n",
    "First, we need to train a Word2Vec model on all the words in our corpus. We will process our wine and food terms separately - some of the wine terms will be standardized to account for commonalities in the colorful language of the world of wine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the most important part: leveraging existing wine theory, the work of others like Bernard Chen, wine descriptor mappings and the UC Davis wine wheel, the top 5000 most frequent wine terms were reviewed to (i) determine whether they are a descriptor that can be derived by blind tasting, and (ii) whether they are informative (judgments like 'tasty' and 'great' are not considered to be informative). The roughly 1000 descriptors that remain were then mapped onto a normalized descriptor, a category and a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_mapping = pd.read_csv('descriptor_mapping.csv', encoding='latin1').set_index('raw descriptor')\n",
    "\n",
    "def return_mapped_descriptor(word, mapping):\n",
    "    if word in list(mapping.index):\n",
    "        normalized_word = mapping.at[word, 'level_3']\n",
    "        return normalized_word\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "normalized_wine_sentences = []\n",
    "for sent in phrased_wine_sentences:\n",
    "    normalized_wine_sentence = []\n",
    "    for word in sent:\n",
    "        normalized_word = return_mapped_descriptor(word, descriptor_mapping)\n",
    "        normalized_wine_sentence.append(str(normalized_word))\n",
    "    normalized_wine_sentences.append(normalized_wine_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through the same process for food, but without normalizing the nonaroma descriptors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aroma_descriptor_mapping = descriptor_mapping.loc[descriptor_mapping['type'] == 'aroma']\n",
    "normalized_food_sentences = []\n",
    "for sent in phrased_food_sentences:\n",
    "    normalized_food_sentence = []\n",
    "    for word in sent:\n",
    "        normalized_word = return_mapped_descriptor(word, aroma_descriptor_mapping)\n",
    "        normalized_food_sentence.append(str(normalized_word))\n",
    "    normalized_food_sentences.append(normalized_food_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's combine the wine dataset with our food dataset so we can train our embeddings. We want to make sure that the food and wine embeddings are calculated in the same feature space so that we can compute similarity vectors later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_sentences = normalized_wine_sentences + normalized_food_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to train our Word2Vec model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_word2vec_model = Word2Vec(normalized_sentences, size=300, min_count=8, iter=15)\n",
    "print(wine_word2vec_model)\n",
    "\n",
    "wine_word2vec_model.save('food_word2vec_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the word2vec model has already been trained, simply load it\n",
    "wine_word2vec_model = Word2Vec.load(\"food_word2vec_model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparing our Food Dataset\n",
    "\n",
    "Now that we have our wine aroma vectors and the nonaroma scalars, we can turn our attention to food. \n",
    "\n",
    "We will want to generate nonaroma vectors for any type of food that we want a wine pairing with. For food, we don't have the luxury of being able to define nonaroma vs. aroma descriptors, so the approach we take will be slightly different:\n",
    "\n",
    "The aroma vector will be the full food embedding.\n",
    "\n",
    "We will define an embedding for each of our core nonaromas (sweet, acid, salt, piquant, fat and bitter), and the weight/body of the food. We will define the maximum distance between each of the nonaroma embeddings and a range of commonly appearing foods. The foods that least and most resemble each nonaroma will eventually allow us to create a normalized scale between 0 (very dissimilar) and 1 (very similar) to say how much a food reflects each nonaroma. \n",
    "\n",
    "First, let's load this list of common foods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods = pd.read_csv('list_of_foods.csv')\n",
    "foods_list = list(foods['Food'])\n",
    "foods_list_normalized = [normalize_text(f) for f in foods_list]\n",
    "foods_list_preprocessed = [food_trigram_model[f][0] for f in foods_list_normalized]\n",
    "foods_list_preprocessed = list(set(foods_list_preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the word embedding for each food in the list of sample foods, and save to a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods_vecs = dict()\n",
    "\n",
    "word_vectors = wine_word2vec_model.wv\n",
    "for f in foods_list_preprocessed:\n",
    "    try:\n",
    "        food_vec = word_vectors[f]\n",
    "        foods_vecs[f] = food_vec\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define the nonaroma embeddings + the weight embedding as the average of foods that represent each nonaroma characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "core_tastes_revised = {'weight': ['heavy', 'cassoulet', 'cassoulet', 'full_bodied', 'thick', 'milk', 'fat', 'mincemeat', 'steak', 'bold', 'pizza', 'pasta', 'creamy', 'bread'],\n",
    "                       'sweet': ['sweet', 'sugar', 'cake', 'mango', 'stevia'], \n",
    "                       'acid': ['acid', 'sour', 'vinegar', 'yoghurt', 'cevich', 'cevich'],\n",
    "                       'salt': ['salty', 'salty', 'parmesan', 'oyster', 'pizza', 'bacon', 'cured_meat', 'sausage', 'potato_chip'], \n",
    "                       'piquant': ['spicy'], \n",
    "                       'fat': ['fat', 'fried', 'creamy', 'cassoulet', 'foie_gras', 'buttery', 'cake', 'foie_gras', 'sausage', 'brie', 'carbonara'], \n",
    "                       'bitter': ['bitter', 'kale']\n",
    "                      }\n",
    "\n",
    "average_taste_vecs = dict()\n",
    "core_tastes_distances = dict()\n",
    "for taste, keywords in core_tastes_revised.items():\n",
    "    \n",
    "    all_keyword_vecs = []\n",
    "    for keyword in keywords:\n",
    "        c_vec = word_vectors[keyword]\n",
    "        all_keyword_vecs.append(c_vec)\n",
    "    \n",
    "    avg_taste_vec = np.average(all_keyword_vecs, axis=0)\n",
    "    average_taste_vecs[taste] = avg_taste_vec\n",
    "        \n",
    "    taste_distances = dict()\n",
    "    for k, v in foods_vecs.items():\n",
    "        similarity = 1- spatial.distance.cosine(avg_taste_vec, v)\n",
    "        taste_distances[k] = similarity\n",
    "        \n",
    "    core_tastes_distances[taste] = taste_distances        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find out which foods most and least resemble each nonaroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "food_nonaroma_infos = dict()\n",
    "# for each core taste, identify the food item that is farthest and closest. We will need this to create a normalized scale between 0 and 1\n",
    "for key, value in core_tastes_revised.items():\n",
    "    dict_taste = dict()\n",
    "    farthest = min(core_tastes_distances[key], key=core_tastes_distances[key].get)\n",
    "    farthest_distance = core_tastes_distances[key][farthest]\n",
    "    closest = max(core_tastes_distances[key], key=core_tastes_distances[key].get)\n",
    "    closest_distance = core_tastes_distances[key][closest]\n",
    "    print(key, farthest, closest)\n",
    "    dict_taste['farthest'] = farthest_distance\n",
    "    dict_taste['closest'] = closest_distance\n",
    "    dict_taste['average_vec'] = average_taste_vecs[key]\n",
    "    food_nonaroma_infos[key] = dict_taste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's save the average embedding for each nonaroma, as well as the minimum and maximum distance to each nonaroma embedding - we will use these to scale the nonaroma scalars that we obtain for any foods we try to pair wine with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_nonaroma_infos_df = pd.DataFrame(food_nonaroma_infos).T\n",
    "food_nonaroma_infos_df.to_csv('average_nonaroma_vectors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the pieces we need to build our wine recommendations. We will continue with this in a separate notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
